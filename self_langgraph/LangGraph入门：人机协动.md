
# ğŸ¤ LangGraph äººæœºååŠ¨ï¼šæ„å»ºå¯ä¸­æ–­ã€å¯å¹²é¢„ã€å¯åä½œçš„æ™ºèƒ½ä½“å·¥ä½œæµ

> **æ ¸å¿ƒæ€æƒ³**ï¼š\
> çœŸæ­£å¯é çš„ AI ç³»ç»Ÿä¸æ˜¯â€œå…¨è‡ªåŠ¨â€ï¼Œè€Œæ˜¯â€œäººåœ¨ç¯è·¯â€ï¼ˆHuman-in-the-Loopï¼‰ã€‚LangGraph é€šè¿‡ **`interrupt()` æœºåˆ¶**ï¼Œå°†äººç±»ä¸“å®¶æ— ç¼åµŒå…¥æ™ºèƒ½ä½“æ‰§è¡Œæµç¨‹ï¼Œå®ç°**å¯æ§ã€å¯ä¿¡ã€å¯å®¡è®¡**çš„åä½œå¼ AIã€‚

***

## ğŸ“Œ ä¸€ã€èƒŒæ™¯ä¸åŠ¨æœºï¼šä¸ºä½•éœ€è¦äººæœºååŠ¨ï¼Ÿ

å½“å‰ LLM æ™ºèƒ½ä½“é¢ä¸´ä¸‰å¤§ä¿¡ä»»ç“¶é¢ˆï¼š

1. **ä¸å¯æ§æ€§**ï¼šä¸€æ—¦å¯åŠ¨ï¼Œæ— æ³•ä¸­é€”å¹²é¢„æˆ–ä¿®æ­£ã€‚
2. **ä¸å¯è§£é‡Šæ€§**ï¼šé»‘ç›’å†³ç­–éš¾ä»¥è¿½æº¯ï¼Œå°¤å…¶åœ¨é«˜é£é™©åœºæ™¯ï¼ˆå¦‚åŒ»ç–—ã€é‡‘èï¼‰ã€‚
3. **çŸ¥è¯†ç›²åŒº**ï¼šLLM æ— æ³•å¤„ç†è¶…å‡ºè®­ç»ƒæ•°æ®çš„ä¸“ä¸šé—®é¢˜ã€‚

LangGraph çš„ **äººæœºååŠ¨ï¼ˆHuman-in-the-Loop Collaborationï¼‰** æœºåˆ¶ï¼Œé€šè¿‡ **æ‰§è¡Œä¸­æ–­ï¼ˆInterruptionï¼‰ + äººå·¥å¹²é¢„ï¼ˆHuman Inputï¼‰ + çŠ¶æ€æ¢å¤ï¼ˆResumptionï¼‰** ä¸‰æ­¥é—­ç¯ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚

> ğŸ’¡ **ç±»æ¯”ç†è§£**ï¼š\
> `interrupt()` â‰ˆ ç¼–ç¨‹ä¸­çš„ `breakpoint()` + ç”¨æˆ·è¾“å…¥å¼¹çª— + æ‰§è¡Œä¸Šä¸‹æ–‡ä¿å­˜


***

## ğŸ§± äºŒã€æ ¸å¿ƒæœºåˆ¶ï¼š`interrupt()` ä¸ `Command`

### 2.1 ä¸­æ–­ï¼šæš‚åœæ‰§è¡Œå¹¶è¯·æ±‚äººå·¥ä»‹å…¥

```python
from langgraph.types import interrupt

def human_assistance(query: str) -&gt; str:
    """è¯·æ±‚äººç±»ä¸“å®¶ååŠ©"""
    # æš‚åœå›¾æ‰§è¡Œï¼Œå¹¶ä¼ é€’ä¸Šä¸‹æ–‡ç»™äººå·¥æ“ä½œå‘˜
    human_response = interrupt({"query": query})
    return human_response["data"]
```

* `interrupt(payload)`ï¼šç«‹å³æš‚åœå½“å‰å›¾æ‰§è¡Œï¼Œä¿å­˜å®Œæ•´çŠ¶æ€ã€‚
* è¿”å›å€¼ä¸ºåç»­æ¢å¤æ—¶ä¼ å…¥çš„ `Command` ä¸­çš„ `resume` æ•°æ®ã€‚

### 2.2 æ¢å¤ï¼šæ³¨å…¥äººå·¥è¾“å…¥å¹¶ç»§ç»­æ‰§è¡Œ

```python
from langgraph.types import Command

# äººå·¥æä¾›çš„ç­”æ¡ˆ
human_response = "We recommend using LangGraph for building reliable AI agents."

# æ„é€ æ¢å¤å‘½ä»¤
human_command = Command(resume={"data": human_response})

# æ¢å¤æ‰§è¡Œï¼ˆè‡ªåŠ¨æ¥ç»­ä¸­æ–­å‰çš„çŠ¶æ€ï¼‰
events = graph.stream(human_command, config, stream_mode="values")
```

> ğŸ” **å…³é”®è®¾è®¡**ï¼š
>
> * `Command` æ˜¯æ¢å¤æ‰§è¡Œçš„**å”¯ä¸€åˆæ³•å…¥å£**ï¼Œç¡®ä¿å¹²é¢„å¯æ§ã€‚
> * `resume` å­—æ®µå†…å®¹å°†ä½œä¸º `interrupt()` çš„è¿”å›å€¼ï¼Œæ— ç¼è¡”æ¥ä¸šåŠ¡é€»è¾‘ã€‚

***

## ğŸ”„ ä¸‰ã€æ„å»ºæ”¯æŒäººæœºååŠ¨çš„æ™ºèƒ½ä½“

### 3.1 çŠ¶æ€ä¸å·¥å…·å®šä¹‰

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

@tool
def human_assistance(query: str) -&gt; str:
    """å·¥å…·ï¼šè¯·æ±‚äººç±»ååŠ©"""
    human_response = interrupt({"query": query})
    return human_response["data"]

# å…¶ä»–å·¥å…·ï¼ˆå¦‚æœç´¢ï¼‰
search_tool = TavilySearch(max_results=2)
tools = [search_tool, human_assistance]
```

### 3.2 å›¾ç»“æ„ï¼šæ”¯æŒè‡ªåŠ¨å·¥å…·ä¸äººå·¥å¹²é¢„

```python
from langgraph.graph import StateGraph, START
from langgraph.prebuilt import ToolNode, tools_condition

graph_builder = StateGraph(State)

# LLM èŠ‚ç‚¹ï¼ˆç»‘å®šå·¥å…·ï¼‰
llm_with_tools = llm.bind_tools(tools)
def chatbot(state: State):
    message = llm_with_tools.invoke(state["messages"])
    assert len(message.tool_calls) &lt;= 1  # é™åˆ¶å•æ¬¡è°ƒç”¨ä¸€ä¸ªå·¥å…·
    return {"messages": [message]}

graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", ToolNode(tools=tools))

# æ§åˆ¶æµ
graph_builder.add_edge(START, "chatbot")
graph_builder.add_conditional_edges("chatbot", tools_condition)
graph_builder.add_edge("tools", "chatbot")
```

> âš ï¸ **æ³¨æ„**ï¼š`assert len(message.tool_calls) <= 1` æ˜¯ä¸ºç®€åŒ–æ¼”ç¤ºã€‚ç”Ÿäº§ç¯å¢ƒå¯æ”¯æŒå¤šå·¥å…·å¹¶è¡Œæˆ–ä¸²è¡Œè°ƒç”¨ã€‚

### 3.3 å¯ç”¨æŒä¹…åŒ–è®°å¿†ï¼ˆæ”¯æŒè·¨ä¸­æ–­ä¼šè¯ï¼‰

```python
from langgraph.checkpoint.memory import InMemorySaver

memory = InMemorySaver()
graph = graph_builder.compile(checkpointer=memory)
```

* ä¸­æ–­æœŸé—´çŠ¶æ€è‡ªåŠ¨ä¿å­˜ï¼Œæ¢å¤æ—¶ä»æ–­ç‚¹ç»§ç»­ã€‚

***

## ğŸ–¼ï¸ å››ã€æ‰§è¡Œæµç¨‹ï¼šä»è¯·æ±‚åˆ°äººå·¥å¹²é¢„å†åˆ°å®Œæˆ

### 4.1 ç”¨æˆ·å‘èµ·è¯·æ±‚ï¼ˆè§¦å‘äººå·¥ååŠ©ï¼‰

```python
user_input = "I need expert guidance for building an AI agent."
config = {"configurable": {"thread_id": "1"}}

events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values"
)
# â†’ LLM è°ƒç”¨ human_assistance å·¥å…· â†’ æ‰§è¡Œä¸­æ–­
```

### 4.2 æ£€æŸ¥ä¸­æ–­çŠ¶æ€

```python
snapshot = graph.get_state(config)
print(snapshot.next)  # è¾“å‡º: ('tools',) 
# â†’ è¡¨ç¤ºä¸‹ä¸€æ­¥éœ€æ‰§è¡Œ tools èŠ‚ç‚¹ï¼ˆå³ç­‰å¾…äººå·¥è¾“å…¥ï¼‰
```

### 4.3 æ³¨å…¥äººå·¥å“åº”å¹¶æ¢å¤

```python
human_response = "Use LangGraphâ€”it's reliable and extensible."
human_command = Command(resume={"data": human_response})

events = graph.stream(human_command, config, stream_mode="values")
for event in events:
    event["messages"][-1].pretty_print()  # è¾“å‡ºæœ€ç»ˆå›ç­”
```

> âœ… **æµç¨‹é—­ç¯**ï¼š
> ç”¨æˆ·è¯·æ±‚ â†’ LLM å†³ç­–éœ€äººå·¥ â†’ ä¸­æ–­ â†’ äººå·¥è¾“å…¥ â†’ æ¢å¤ â†’ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

***

## ğŸ“Š äº”ã€äººæœºååŠ¨ vs å…¨è‡ªåŠ¨æ™ºèƒ½ä½“å¯¹æ¯”

| èƒ½åŠ› | å…¨è‡ªåŠ¨æ™ºèƒ½ä½“ | äººæœºååŠ¨æ™ºèƒ½ä½“ |
|------|---------------|------------------|
| **å¯æ§æ€§** | ä½ï¼ˆé»‘ç›’æ‰§è¡Œï¼‰ | é«˜ï¼ˆéšæ—¶ä¸­æ–­å¹²é¢„ï¼‰ |
| **å¯é æ€§** | ä¾èµ– LLM èƒ½åŠ› | äººç±»å…œåº•ï¼Œé”™è¯¯ç‡â†“ |
| **é€‚ç”¨åœºæ™¯** | ä½é£é™©ã€æ ‡å‡†åŒ–ä»»åŠ¡ | é«˜é£é™©ã€ä¸“ä¸šé¢†åŸŸã€åˆè§„åœºæ™¯ |
| **å®¡è®¡è¿½è¸ª** | å›°éš¾ | å®Œæ•´è®°å½•äººå·¥ä»‹å…¥ç‚¹ |
| **å¼€å‘è°ƒè¯•** | ä¾èµ–æ—¥å¿—å›æº¯ | å®æ—¶äº¤äº’å¼è°ƒè¯• |

***

## ğŸ¯ å…­ã€å…¸å‹åº”ç”¨åœºæ™¯

1. **é‡‘èé£æ§**ï¼šå¤§é¢è½¬è´¦å‰è¯·æ±‚äººå·¥å¤æ ¸ã€‚
2. **åŒ»ç–—å’¨è¯¢**ï¼šé‡åˆ°ç½•è§ç—…æ—¶è½¬æ¥åŒ»ç”Ÿã€‚
3. **ä»£ç ç”Ÿæˆ**ï¼šç”Ÿæˆå…³é”®æ¨¡å—å‰è¯·æ±‚å¼€å‘è€…ç¡®è®¤ã€‚
4. **å†…å®¹å®¡æ ¸**ï¼šæ•æ„Ÿå†…å®¹äº¤ç”±å®¡æ ¸å‘˜åˆ¤æ–­ã€‚
5. **AI æ•™å­¦**ï¼šå­¦ç”Ÿè¯·æ±‚â€œæç¤ºâ€æ—¶æš‚åœå¹¶æä¾›å¼•å¯¼ã€‚

***

## ğŸ› ï¸ ä¸ƒã€å·¥ç¨‹æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

* **æ˜ç¡®ä¸­æ–­è§¦å‘æ¡ä»¶**ï¼šä»…åœ¨é«˜ä¸ç¡®å®šæ€§æˆ–é«˜é£é™©æ—¶è°ƒç”¨ `human_assistance`ã€‚
* **ç»“æ„åŒ–ä¼ é€’ä¸Šä¸‹æ–‡**ï¼š`interrupt({"query": ..., "context": ..., "risk_level": ...})`ã€‚
* **è®¾ç½®è¶…æ—¶æ¢å¤æœºåˆ¶**ï¼šé¿å…äººå·¥é•¿æ—¶é—´æœªå“åº”å¯¼è‡´æµç¨‹å¡æ­»ï¼ˆéœ€è‡ªå®šä¹‰è°ƒåº¦å™¨ï¼‰ã€‚
* **è®°å½•å¹²é¢„æ—¥å¿—**ï¼šç”¨äºåç»­åˆ†æä¸æ¨¡å‹è¿­ä»£ã€‚

### âŒ é¿å…è¯¯åŒº

* **è¯¯åŒº1**ï¼šâ€œäººå·¥å¹²é¢„ = é™ä½è‡ªåŠ¨åŒ–â€\
  â†’ å®é™…æ˜¯**æå‡ç³»ç»Ÿæ•´ä½“å¯é æ€§**ï¼Œå®ç°â€œè‡ªåŠ¨åŒ–ä¸ºä¸»ï¼Œäººå·¥å…œåº•â€ã€‚

* **è¯¯åŒº2**ï¼šâ€œä¸­æ–­åçŠ¶æ€ä¸¢å¤±â€\
  â†’ LangGraph **è‡ªåŠ¨æŒä¹…åŒ–å®Œæ•´çŠ¶æ€**ï¼Œæ¢å¤åä¸Šä¸‹æ–‡æ— ç¼è¡”æ¥ã€‚


***

## ğŸ“ é™„å½•ï¼šå®Œæ•´ä»£ç ç»“æ„ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

```python
# 1. åˆå§‹åŒ–
llm = ChatDeepSeek(model="deepseek-chat", api_key=os.getenv("DEEPSEEK_API_KEY"))
search_tool = TavilySearch(max_results=2)

# 2. å®šä¹‰äººå·¥ååŠ©å·¥å…·
@tool
def human_assistance(query: str) -&gt; str:
    response = interrupt({"query": query})
    return response["data"]

tools = [search_tool, human_assistance]

# 3. æ„å»ºå›¾
class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    msg = llm_with_tools.invoke(state["messages"])
    assert len(msg.tool_calls) &lt;= 1
    return {"messages": [msg]}

graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", ToolNode(tools=tools))
graph_builder.set_entry_point("chatbot")
graph_builder.add_conditional_edges("chatbot", tools_condition)
graph_builder.add_edge("tools", "chatbot")

# 4. ç¼–è¯‘ï¼ˆå¯ç”¨è®°å¿†ï¼‰
graph = graph_builder.compile(checkpointer=InMemorySaver())

# 5. æ‰§è¡Œä¸å¹²é¢„
config = {"configurable": {"thread_id": "1"}}
graph.stream({"messages": [{"role": "user", "content": "Need expert help!"}]}, config)

# 6. æ¢å¤ï¼ˆäººå·¥è¾“å…¥åï¼‰
human_cmd = Command(resume={"data": "Use LangGraph."})
graph.stream(human_cmd, config)
```


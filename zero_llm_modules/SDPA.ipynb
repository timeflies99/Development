{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attn_mask: torch.Tensor = None,\n",
    "    dropout_p: float = 0.0,\n",
    "    is_causal: bool = False,  # 是否使用因果掩码（例如在Decoder中）\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    实现缩放点积注意力 (Scaled Dot-Product Attention)。\n",
    "\n",
    "    Args:\n",
    "        query (torch.Tensor): 查询张量，形状通常为 (..., query_seq_len, head_dim)。\n",
    "                            ... 可以是 batch_size, num_heads 等。\n",
    "        key (torch.Tensor): 键张量，形状通常为 (..., key_seq_len, head_dim)。\n",
    "        value (torch.Tensor): 值张量，形状通常为 (..., key_seq_len, value_dim)。\n",
    "                            key_seq_len 和 value_seq_len 必须相同。\n",
    "        attn_mask (torch.Tensor, optional): 注意力掩码。\n",
    "                                        形状可以是 (query_seq_len, key_seq_len) 或\n",
    "                                        (batch_size, query_seq_len, key_seq_len) 或\n",
    "                                        (batch_size, num_heads, query_seq_len, key_seq_len)。\n",
    "                                        True 表示屏蔽（不关注），False 表示可见。\n",
    "                                        注意：PyTorch 的 `masked_fill` 通常用 True 表示要填充的值。\n",
    "                                        这里我们假设输入的 mask 是标准的 `True` 屏蔽。\n",
    "        dropout_p (float, optional): dropout 概率。默认为 0.0。\n",
    "        is_causal (bool, optional): 如果为 True，则自动生成一个因果（下三角）注意力掩码。\n",
    "                                    用于自回归任务，确保当前 token 只能关注之前的 token。\n",
    "                                    如果提供了 attn_mask 且 is_causal 为 True，则两者会合并。\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 注意力机制的输出，形状与 query 相同，但最后一个维度是 value_dim。\n",
    "                    (..., query_seq_len, value_dim)\n",
    "    \"\"\"\n",
    "    L, S = query.size(-2), key.size(-2)  # L: query_seq_len, S: key_seq_len\n",
    "    head_dim = query.size(-1)  # d_k\n",
    "\n",
    "    # 1. 计算 QK^T\n",
    "    # (..., L, D) @ (..., D, S) -> (..., L, S)\n",
    "    attn_scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "    # 2. 缩放\n",
    "    attn_scores = attn_scores / (head_dim**0.5)\n",
    "\n",
    "    # 3. 应用因果掩码 (如果 is_causal 为 True)\n",
    "    if is_causal:\n",
    "        # 创建一个上三角掩码，确保当前位置只能关注之前或自己的位置\n",
    "        # (L, S) 的形状，对角线及以下为 True (可见)，对角线以上为 False (不可见)\n",
    "        # masked_fill 需要 True 表示要屏蔽的部分，所以我们用相反的逻辑\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(L, S, dtype=torch.bool, device=query.device), diagonal=1\n",
    "        )\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask, float(\"-inf\"))\n",
    "\n",
    "    # 4. 应用外部提供的注意力掩码 (如果存在)\n",
    "    if attn_mask is not None:\n",
    "        # PyTorch 的 masked_fill 通常用 True 表示要填充的部分（例如 -inf），False 表示不填充\n",
    "        # 所以如果你的 attn_mask 是 True 表示屏蔽，False 表示可见，则可以直接使用\n",
    "        # 如果你的 mask 是 0 表示屏蔽，1 表示可见，则需要 mask == 0\n",
    "        attn_scores = attn_scores.masked_fill(attn_mask, float(\"-inf\"))\n",
    "\n",
    "    # 5. Softmax 归一化\n",
    "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "    # 6. Dropout\n",
    "    attn_weights = F.dropout(attn_weights, p=dropout_p)\n",
    "\n",
    "    # 7. 与 Value 矩阵相乘\n",
    "    # (..., L, S) @ (..., S, D_v) -> (..., L, D_v)\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 1: 基本用法 (无掩码，无 dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 示例 1: 基本用法 ---\n",
      "Query Shape: torch.Size([2, 3, 64])\n",
      "Key Shape: torch.Size([2, 5, 64])\n",
      "Value Shape: torch.Size([2, 5, 128])\n",
      "Output Shape: torch.Size([2, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "# 示例 1: 基本用法 (无掩码，无 dropout)\n",
    "print(\"--- 示例 1: 基本用法 ---\")\n",
    "batch_size = 2\n",
    "seq_len_q = 3\n",
    "seq_len_kv = 5\n",
    "head_dim = 64\n",
    "value_dim = 128  # value_dim 可以与 head_dim 不同\n",
    "\n",
    "query = torch.randn(batch_size, seq_len_q, head_dim)\n",
    "key = torch.randn(batch_size, seq_len_kv, head_dim)\n",
    "value = torch.randn(batch_size, seq_len_kv, value_dim)\n",
    "\n",
    "output = scaled_dot_product_attention(query, key, value)\n",
    "print(f\"Query Shape: {query.shape}\")\n",
    "print(f\"Key Shape: {key.shape}\")\n",
    "print(f\"Value Shape: {value.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "# 期望：\n",
    "# Query Shape: torch.Size([2, 3, 64])\n",
    "# Key Shape: torch.Size([2, 5, 64])\n",
    "# Value Shape: torch.Size([2, 5, 128])\n",
    "# Output Shape: torch.Size([2, 3, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 2: 使用注意力掩码 (例如，填充掩码)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- 示例 2: 使用注意力掩码 ---\")\n",
    "# 模拟一个填充掩码\n",
    "# 假设 batch_size=2，seq_len_kv=5\n",
    "# 第一个样本的有效长度是 3，后面两个是填充\n",
    "# 第二个样本的有效长度是 4，最后一个是填充\n",
    "key_padding_mask_bool = torch.tensor(\n",
    "    [[False, False, False, True, True], [False, False, False, False, True]],\n",
    "    dtype=torch.bool,\n",
    ")  # True 表示需要屏蔽的部分\n",
    "\n",
    "# 广播到注意力分数维度: (batch_size, 1, key_seq_len) -> (batch_size, query_seq_len, key_seq_len)\n",
    "# 如果是多头，可能需要 (batch_size, num_heads, 1, key_seq_len)\n",
    "# 这里假设 attn_mask 是 (batch_size, query_seq_len, key_seq_len)\n",
    "# 或者可以直接让 mask 能够广播到 attn_scores (..., L, S)\n",
    "attn_mask_expanded = key_padding_mask_bool.unsqueeze(1).expand(-1, seq_len_q, -1)\n",
    "\n",
    "output_with_mask = scaled_dot_product_attention(\n",
    "    query, key, value, attn_mask=attn_mask_expanded\n",
    ")\n",
    "print(f\"Output with Mask Shape: {output_with_mask.shape}\")\n",
    "# 形状不变，但内部值会因为掩码而改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 3: 使用因果掩码 (在自注意力中常见)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- 示例 3: 使用因果掩码 ---\")\n",
    "# 自注意力通常 Q, K, V 来自同一个源\n",
    "query_self = torch.randn(batch_size, seq_len_kv, head_dim)\n",
    "key_self = query_self.clone()\n",
    "value_self = query_self.clone()\n",
    "\n",
    "# is_causal=True 会自动生成一个下三角掩码\n",
    "output_causal = scaled_dot_product_attention(\n",
    "    query_self, key_self, value_self, is_causal=True\n",
    ")\n",
    "print(f\"Output with Causal Mask Shape: {output_causal.shape}\")\n",
    "# 期望：torch.Size([2, 5, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例 4: 结合多头情况下的 SDPA (MHA 的一部分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例 4: 结合多头情况下的 SDPA (MHA 的一部分)\n",
    "print(\"\\n--- 示例 4: 结合多头情况下的 SDPA ---\")\n",
    "num_heads = 8\n",
    "total_embed_dim = 512\n",
    "head_dim = total_embed_dim // num_heads  # 64\n",
    "\n",
    "# 假设 Q, K, V 已经通过线性层并被分割成多头\n",
    "# 形状通常变为 (batch_size, num_heads, seq_len, head_dim)\n",
    "query_mha = torch.randn(batch_size, num_heads, seq_len_q, head_dim)\n",
    "key_mha = torch.randn(batch_size, num_heads, seq_len_kv, head_dim)\n",
    "value_mha = torch.randn(batch_size, num_heads, seq_len_kv, head_dim)\n",
    "\n",
    "# 直接传入到 SDPA 函数，因为 SDPA 可以处理 ... (任意批次维度)\n",
    "output_mha_part = scaled_dot_product_attention(query_mha, key_mha, value_mha)\n",
    "print(f\"Output for a single head calculation in MHA: {output_mha_part.shape}\")\n",
    "# 期望：torch.Size([2, 8, 3, 64])\n",
    "# 之后这些头的输出会被拼接并再次线性变换。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
